{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카트폴 게임 마스터하기\n",
    "\n",
    "어떤 게임을 마스터한다는 뜻은 최고의 점수를 받는다는 뜻이기도 합니다.\n",
    "그러므로 게임의 점수를 리워드로 취급하면 될 것 같습니다.\n",
    "우리가 만들 에이전트는 리워드를 예측하고,\n",
    "리워드를 최대로 만드는 쪽으로 학습하게 할 것입니다.\n",
    "\n",
    "예를들어 카트폴 게임에서는 막대기를 세우고 오래 버틸수록 점수가 증가합니다.\n",
    "카트폴 게임에서 막대가 오른쪽으로 기울었을때,\n",
    "어느 동작이 가장 큰 리워드를 준다고 예측할 수 있을까요?\n",
    "오른쪽으로 가서 중심을 다시 맞춰야 하니\n",
    "오른쪽 버튼을 누르는 쪽이 왼쪽 버튼보다 리워드가 클 것이라고 예측 할 수 있습니다.\n",
    "\n",
    "이것을 한줄로 요약하자면 아래 한줄의 코드가 됩니다.\n",
    "\n",
    "```\n",
    "target = reward + gamma * np.amax(model.predict(next_state))\n",
    "```\n",
    "\n",
    "DQN은 가장 중요한 특징 2가지로 요약될 수 있습니다.\n",
    "바로 기억하기(Remember)와 다시 보기(Replay)입니다.\n",
    "둘다 간단한 아이디어이지만 신경망이 강화학습에 이용될 수 있게 만든 혁명적인 방법들입니다.\n",
    "순서대로 개념과 구현법을 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym을 이용하여 게임환경 구축하기\n",
    "\n",
    "\n",
    "강화학습 예제들을 보면 항상 게임과 연관되어 있습니다. 원래 우리가 궁극적으로 원하는 목표는 어디서든 적응할 수 있는 인공지능이지만, 너무 복잡한 문제이기도 하고 가상 환경을 설계하기도 어렵기 때문에 일단 게임이라는 환경을 사용해 하는 것입니다.\n",
    "\n",
    "대부분의 게임은 점수 혹은 목표가 있습니다. 점수가 오르거나 목표에 도달하면 일종의 리워드를 받고 원치 않은 행동을 할때는 마이너스 리워드를 주는 경우도 있습니다. 아까 비유를 들었던 달리기를 배울때의 경우를 예로 들면 총 나아간 길이 혹은 목표 도착지 도착 여부로 리워드를 주고 넘어질때 패널티를 줄 수 있을 것입니다. \n",
    "\n",
    "게임중에서도 가장 간단한 카트폴이라는 환경을 구축하여 강화학습을 배울 토대를 마련해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPISODES = 50  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "GAMMA = 0.8  # Q-learning discount factor\n",
    "LR = 0.001  # NN optimizer learning rate\n",
    "BATCH_SIZE = 64  # Q-learning batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 에이전트\n",
    "\n",
    "DQNAgent라는 클래스를 만들어 \n",
    "\n",
    "```python\n",
    "class DQNAgent\n",
    "```\n",
    "\n",
    "### DQN 에이전트의 뇌, 뉴럴넷\n",
    "\n",
    "```python\n",
    "self.model = nn.Sequential(\n",
    "    nn.Linear(4, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 2)\n",
    ")\n",
    "```\n",
    "\n",
    "### 행동하기 (Act)\n",
    "\n",
    "\n",
    "\n",
    "### 전 경험 기억하기 (Remember)\n",
    "\n",
    "신경망을 Q-learning학습에 처음 적용하면서 맞닥뜨린 문제는\n",
    "바로 신경망이 새로운 경험을 전 경험에 겹쳐쓰며 쉽게 잊어버린다는 것이었습니다.\n",
    "그래서 나온 해결책이 바로 기억하기(Remember)라는 기능인데요,\n",
    "바로 이전 경험들을 배열에 담아 계속 재학습 시키며 신경망이 까먹지 않게 하는 아이디어 입니다. \n",
    "\n",
    "각 경험은 상태, 행동, 보상등을 담아야 합니다.\n",
    "이전 경험들을 담을 배열을 `memory`라고 부르고 아래와 같이 만들어봅시다.\n",
    "\n",
    "```python\n",
    "self.memory = [(상태, 행동, 보상, 다음 상태)...]\n",
    "```\n",
    "이를 구현하기 위해 복잡한 모델을 만들때는 Memory클래스를 구현하기도 하지만,\n",
    "이번 예제에서는 사용하기 가장 간단한 deque (double ended queue),\n",
    "즉 큐(queue) 자료구조를 이용할 것입니다.\n",
    "파이썬에서 `deque`의 `maxlen`을 지정해주었을때 큐가 가득 찼을 경우\n",
    "제일 오래된 요소부터 없어지므로\n",
    "자연스레 오래된 기억을 까먹게 해주는 역할을 할 수 있습니다.\n",
    "\n",
    "```python\n",
    "self.memory = deque(maxlen=10000)\n",
    "```\n",
    "\n",
    "\n",
    "그리고 memory 배열에 새로운 경험을 덧붙일 remember() 함수를 만들어보겠습니다.\n",
    "\n",
    "```python\n",
    "def memorize(self, state, action, reward, next_state):\n",
    "    self.memory.append((state,\n",
    "                        action,\n",
    "                        torch.FloatTensor([reward]),\n",
    "                        torch.FloatTensor([next_state])))\n",
    "```\n",
    "\n",
    "### 경험으로부터 배우기 (Experience Replay)\n",
    "\n",
    "이전 경험들을 모아놨으면 반복적으로 학습해야합니다.\n",
    "사람도 수면중일때 자동차 운전, 농구 슈팅,\n",
    "등 운동과 관련된 정보를 정리하며,\n",
    "단기 기억을 운동피질에서 측두엽으로 전달하여 장기 기억으로 변환시킨다고 합니다.\n",
    "우연하게도 DQN에이전트가 기억하고 다시 상기하는 과정도 비슷한 개념입니다.\n",
    "\n",
    "`learn`함수는 바로 이런 개념으로 방금 만들어둔 뉴럴넷인 `model`을\n",
    "`memory`에 쌓인 경험을 토대로 학습시키는 역할을 합니다.\n",
    "\n",
    "```python\n",
    "   def learn(self):\n",
    "        \"\"\"Experience Replay\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "```\n",
    "\n",
    "`self.memory`에서 무작위로 배치 크기만큼의 \"경험\"들을 가져옵니다.\n",
    "이 예제에선 배치사이즈를 64개로 정했습니다.\n",
    "\n",
    "```python\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "```\n",
    "\n",
    "각각의 경험들은 상태(`states`), 행동(`actions`), 행동에 따른 보상(`rewards`),\n",
    "그리고 다음 상태(`next_states`)를 담고있습니다.\n",
    "모두 리스트의 리스트 형태이므로 `torch.cat()`을 이용하여 하나의 리스트로 만듭니다.\n",
    "`cat`은 concatenate의 준말로 결합하다, 혹은 연결하다라는 뜻입니다.\n",
    "\n",
    "```python\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (GAMMA * max_next_q)\n",
    "```\n",
    "\n",
    "Q값을 구합니다.\n",
    "\n",
    "```python\n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "```\n",
    "\n",
    "학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
    "        self.steps_done = 0\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if random.random() > eps_threshold:\n",
    "            return self.model(state).data.max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.LongTensor([[random.randrange(2)]])\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append((state,\n",
    "                            action,\n",
    "                            torch.FloatTensor([reward]),\n",
    "                            torch.FloatTensor([next_state])))\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Experience Replay\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (GAMMA * max_next_q)\n",
    "        \n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 준비하기\n",
    "\n",
    "드디어 만들어둔 DQNAgent를 인스턴스화 합니다.\n",
    "그리고 `gym`을 이용하여 `CartPole-v0`환경도 준비합니다.\n",
    "자, 이제 `agent` 객체를 이용하여 `CartPole-v0` 환경과 상호작용을 통해 게임을 배우도록 하겠습니다.\n",
    "학습 진행을 기록하기 위해 `score_history` 리스트를 이용하여 점수를 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "env = gym.make('CartPole-v0')\n",
    "score_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 시작하기\n",
    "\n",
    "EPISODES는 얼마나 많은 게임을 진행하느냐를 나타내는 하이퍼파라미터입니다.\n",
    "\n",
    "```\n",
    "for e in range(1, EPISODES+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "```\n",
    "\n",
    "`done`변수에는 게임이 끝났는지의 여부가 참(True), 거짓(False)로 표현됩니다.\n",
    "\n",
    "```\n",
    "    while True:\n",
    "        env.render()\n",
    "        state = torch.FloatTensor([state])\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "```\n",
    "\n",
    "우리의 에이전트가 한 행동의 결과가 나왔습니다!\n",
    "이 경험을 기억(memorize)하고 배우도록합니다.\n",
    "\n",
    "```\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "```\n",
    "\n",
    "게임이 끝났을 경우 `done`이 `True`가 되며 아래 코드가 실행되게 됩니다.\n",
    "보통 게임 분석을 위해 복잡한 도구와 코드가 사용되는 경우가 많으나\n",
    "여기서는 간단하게 에피소드 숫자와 점수만 표기하도록 하겠습니다.\n",
    "또 앞서 만들어둔 `score_history` 리스트에 점수를 담도록 합니다.\n",
    "마지막으로 게임이 더 이상 진행되지 않으므로 `break` 문으로 무한루프를 나옵니다.\n",
    "\n",
    "```\n",
    "        if done:\n",
    "            print(\"에피소드:{0} 점수: {1}\".format(e, steps))\n",
    "            score_history.append(steps)\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[99m Episode 1 finished after 36 steps\n",
      "\u001b[99m Episode 2 finished after 59 steps\n",
      "\u001b[99m Episode 3 finished after 27 steps\n",
      "\u001b[99m Episode 4 finished after 14 steps\n",
      "\u001b[99m Episode 5 finished after 20 steps\n",
      "\u001b[99m Episode 6 finished after 10 steps\n",
      "\u001b[99m Episode 7 finished after 26 steps\n",
      "\u001b[99m Episode 8 finished after 16 steps\n",
      "\u001b[99m Episode 9 finished after 15 steps\n",
      "\u001b[99m Episode 10 finished after 10 steps\n",
      "\u001b[99m Episode 11 finished after 12 steps\n",
      "\u001b[99m Episode 12 finished after 14 steps\n",
      "\u001b[99m Episode 13 finished after 12 steps\n",
      "\u001b[99m Episode 14 finished after 10 steps\n",
      "\u001b[99m Episode 15 finished after 9 steps\n",
      "\u001b[99m Episode 16 finished after 15 steps\n",
      "\u001b[99m Episode 17 finished after 17 steps\n",
      "\u001b[99m Episode 18 finished after 39 steps\n",
      "\u001b[99m Episode 19 finished after 13 steps\n",
      "\u001b[99m Episode 20 finished after 15 steps\n",
      "\u001b[92m Episode 21 finished after 200 steps\n",
      "\u001b[99m Episode 22 finished after 90 steps\n",
      "\u001b[99m Episode 23 finished after 82 steps\n",
      "\u001b[99m Episode 24 finished after 99 steps\n",
      "\u001b[99m Episode 25 finished after 103 steps\n",
      "\u001b[99m Episode 26 finished after 83 steps\n",
      "\u001b[99m Episode 27 finished after 93 steps\n",
      "\u001b[99m Episode 28 finished after 146 steps\n",
      "\u001b[99m Episode 29 finished after 114 steps\n",
      "\u001b[99m Episode 30 finished after 173 steps\n",
      "\u001b[99m Episode 31 finished after 117 steps\n",
      "\u001b[99m Episode 32 finished after 139 steps\n",
      "\u001b[99m Episode 33 finished after 153 steps\n",
      "\u001b[99m Episode 34 finished after 128 steps\n",
      "\u001b[99m Episode 35 finished after 124 steps\n",
      "\u001b[99m Episode 36 finished after 141 steps\n",
      "\u001b[99m Episode 37 finished after 175 steps\n",
      "\u001b[99m Episode 38 finished after 179 steps\n",
      "\u001b[99m Episode 39 finished after 130 steps\n",
      "\u001b[99m Episode 40 finished after 122 steps\n",
      "\u001b[99m Episode 41 finished after 182 steps\n",
      "\u001b[99m Episode 42 finished after 129 steps\n",
      "\u001b[99m Episode 43 finished after 144 steps\n",
      "\u001b[99m Episode 44 finished after 125 steps\n",
      "\u001b[99m Episode 45 finished after 128 steps\n",
      "\u001b[99m Episode 46 finished after 142 steps\n",
      "\u001b[99m Episode 47 finished after 153 steps\n",
      "\u001b[99m Episode 48 finished after 160 steps\n",
      "\u001b[92m Episode 49 finished after 200 steps\n",
      "\u001b[99m Episode 50 finished after 132 steps\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, EPISODES+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        state = torch.FloatTensor([state])\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"에피소드:{0} 점수: {1}\".format(e, steps))\n",
    "            score_history.append(steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
