{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카트폴 게임 마스터하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "EPISODES = 50    # 애피소드 반복횟수\n",
    "EPS_START = 0.9  # 학습 시작시 에이전트가 무작위로 행동할 확률\n",
    "EPS_END = 0.05   # 학습 막바지에 에이전트가 무작위로 행동할 확률\n",
    "EPS_DECAY = 200  # 학습 진행시 에이전트가 무작위로 행동할 확률을 감소시키는 값\n",
    "GAMMA = 0.8      # 할인계수\n",
    "LR = 0.001       # 학습률\n",
    "BATCH_SIZE = 64  # 배치 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
    "        self.steps_done = 0\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append((state,\n",
    "                            action,\n",
    "                            torch.FloatTensor([reward]),\n",
    "                            torch.FloatTensor([next_state])))\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if random.random() > eps_threshold:\n",
    "            return self.model(state).data.max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.LongTensor([[random.randrange(2)]])\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Experience Replay\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (GAMMA * max_next_q)\n",
    "        \n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 준비하기\n",
    "\n",
    "`gym`을 이용하여 `CartPole-v0`환경을 준비하고 앞서 만들어둔 DQNAgent를 agent로 인스턴스화 합니다.\n",
    "\n",
    "자, 이제 `agent` 객체를 이용하여 `CartPole-v0` 환경과 상호작용을 통해 게임을 배우도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = DQNAgent()\n",
    "score_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드:1 점수: 21\n",
      "에피소드:2 점수: 28\n",
      "에피소드:3 점수: 21\n",
      "에피소드:4 점수: 51\n",
      "에피소드:5 점수: 12\n",
      "에피소드:6 점수: 20\n",
      "에피소드:7 점수: 8\n",
      "에피소드:8 점수: 9\n",
      "에피소드:9 점수: 10\n",
      "에피소드:10 점수: 12\n",
      "에피소드:11 점수: 14\n",
      "에피소드:12 점수: 11\n",
      "에피소드:13 점수: 9\n",
      "에피소드:14 점수: 9\n",
      "에피소드:15 점수: 10\n",
      "에피소드:16 점수: 26\n",
      "에피소드:17 점수: 11\n",
      "에피소드:18 점수: 9\n",
      "에피소드:19 점수: 11\n",
      "에피소드:20 점수: 25\n",
      "에피소드:21 점수: 12\n",
      "에피소드:22 점수: 19\n",
      "에피소드:23 점수: 12\n",
      "에피소드:24 점수: 27\n",
      "에피소드:25 점수: 30\n",
      "에피소드:26 점수: 66\n",
      "에피소드:27 점수: 24\n",
      "에피소드:28 점수: 45\n",
      "에피소드:29 점수: 47\n",
      "에피소드:30 점수: 35\n",
      "에피소드:31 점수: 35\n",
      "에피소드:32 점수: 40\n",
      "에피소드:33 점수: 44\n",
      "에피소드:34 점수: 34\n",
      "에피소드:35 점수: 57\n",
      "에피소드:36 점수: 52\n",
      "에피소드:37 점수: 70\n",
      "에피소드:38 점수: 124\n",
      "에피소드:39 점수: 118\n",
      "에피소드:40 점수: 33\n",
      "에피소드:41 점수: 128\n",
      "에피소드:42 점수: 55\n",
      "에피소드:43 점수: 178\n",
      "에피소드:44 점수: 88\n",
      "에피소드:45 점수: 103\n",
      "에피소드:46 점수: 101\n",
      "에피소드:47 점수: 120\n",
      "에피소드:48 점수: 140\n",
      "에피소드:49 점수: 113\n",
      "에피소드:50 점수: 85\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, EPISODES+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        state = torch.FloatTensor([state])\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"에피소드:{0} 점수: {1}\".format(e, steps))\n",
    "            score_history.append(steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 28, 21, 51, 12, 20, 8, 9, 10, 12, 14, 11, 9, 9, 10, 26, 11, 9, 11, 25, 12, 19, 12, 27, 30, 66, 24, 45, 47, 35, 35, 40, 44, 34, 57, 52, 70, 124, 118, 33, 128, 55, 178, 88, 103, 101, 120, 140, 113, 85]\n"
     ]
    }
   ],
   "source": [
    "print(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
